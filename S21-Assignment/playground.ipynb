{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2 import DataLoaderLite, TrainGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338025 tokens\n",
      "1 epoch = 41 batches\n",
      "using device cuda\n"
     ]
    }
   ],
   "source": [
    "# Load Dataloader\n",
    "train_loader = DataLoaderLite(B=8, T=1024, file='input.txt')\n",
    "trainer = TrainGPT(model_compile=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repos\\ERA_V2\\S21-Assignment\\gpt2.py:65: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: 11.030921936035156, dt: 428.05ms tok/sec: 19137.86\n",
      "Step 100, loss: 5.84446907043457, dt: 157.00ms tok/sec: 52177.69\n",
      "Step 200, loss: 5.322109222412109, dt: 166.00ms tok/sec: 49348.23\n",
      "Step 300, loss: 5.2536139488220215, dt: 161.00ms tok/sec: 50881.98\n",
      "Step 400, loss: 4.880702018737793, dt: 161.62ms tok/sec: 50685.78\n",
      "Step 500, loss: 4.45494270324707, dt: 164.12ms tok/sec: 49915.29\n",
      "Step 600, loss: 4.921161651611328, dt: 162.00ms tok/sec: 50567.99\n",
      "Step 700, loss: 4.42236852645874, dt: 161.00ms tok/sec: 50882.66\n",
      "Step 800, loss: 4.441651821136475, dt: 160.00ms tok/sec: 51200.05\n",
      "Step 900, loss: 4.115748405456543, dt: 161.00ms tok/sec: 50882.28\n",
      "Step 1000, loss: 4.408835411071777, dt: 161.51ms tok/sec: 50722.59\n",
      "Step 1100, loss: 4.0244550704956055, dt: 162.00ms tok/sec: 50567.85\n",
      "Step 1200, loss: 4.29283332824707, dt: 161.12ms tok/sec: 50843.06\n",
      "Step 1300, loss: 4.194064140319824, dt: 161.00ms tok/sec: 50881.23\n",
      "Step 1400, loss: 4.005340576171875, dt: 162.51ms tok/sec: 50409.67\n",
      "Step 1500, loss: 3.9327304363250732, dt: 162.00ms tok/sec: 50567.99\n",
      "Step 1600, loss: 4.026634216308594, dt: 162.00ms tok/sec: 50567.85\n",
      "Step 1700, loss: 3.704533100128174, dt: 168.00ms tok/sec: 48761.63\n",
      "Step 1800, loss: 3.5162887573242188, dt: 162.65ms tok/sec: 50367.11\n",
      "Step 1900, loss: 3.6213812828063965, dt: 163.65ms tok/sec: 50057.24\n",
      "Step 2000, loss: 3.567453145980835, dt: 169.13ms tok/sec: 48435.75\n",
      "Step 2100, loss: 3.4217472076416016, dt: 164.03ms tok/sec: 49940.75\n",
      "Step 2200, loss: 3.6989176273345947, dt: 171.78ms tok/sec: 47690.26\n",
      "Step 2300, loss: 3.1883139610290527, dt: 167.74ms tok/sec: 48836.62\n",
      "Step 2400, loss: 3.4506216049194336, dt: 164.13ms tok/sec: 49912.46\n",
      "Step 2500, loss: 3.1528217792510986, dt: 168.63ms tok/sec: 48578.94\n",
      "Step 2600, loss: 3.010504961013794, dt: 165.00ms tok/sec: 49648.35\n",
      "Step 2700, loss: 2.761927366256714, dt: 161.12ms tok/sec: 50844.56\n",
      "Step 2800, loss: 2.929175615310669, dt: 161.00ms tok/sec: 50881.91\n",
      "Step 2900, loss: 2.7659425735473633, dt: 160.01ms tok/sec: 51198.14\n",
      "Step 3000, loss: 2.6595818996429443, dt: 160.59ms tok/sec: 51011.23\n",
      "Step 3100, loss: 2.407015085220337, dt: 161.00ms tok/sec: 50882.06\n",
      "Step 3200, loss: 2.3413479328155518, dt: 160.36ms tok/sec: 51084.19\n",
      "Step 3300, loss: 2.130338191986084, dt: 161.00ms tok/sec: 50882.06\n",
      "Step 3400, loss: 1.9854280948638916, dt: 161.00ms tok/sec: 50882.06\n",
      "Step 3500, loss: 1.8285661935806274, dt: 162.00ms tok/sec: 50568.37\n",
      "Step 3600, loss: 1.919366717338562, dt: 160.00ms tok/sec: 51200.13\n",
      "Step 3700, loss: 1.5700737237930298, dt: 161.00ms tok/sec: 50881.76\n",
      "Step 3800, loss: 1.7983462810516357, dt: 161.00ms tok/sec: 50881.98\n",
      "Step 3900, loss: 1.4459023475646973, dt: 160.00ms tok/sec: 51199.97\n",
      "Step 4000, loss: 1.3075239658355713, dt: 160.00ms tok/sec: 51199.90\n",
      "Step 4100, loss: 1.4234212636947632, dt: 161.00ms tok/sec: 50882.28\n",
      "Step 4200, loss: 1.0964140892028809, dt: 162.51ms tok/sec: 50409.90\n",
      "Step 4300, loss: 1.1935838460922241, dt: 161.00ms tok/sec: 50881.23\n",
      "Step 4400, loss: 0.9183558821678162, dt: 161.00ms tok/sec: 50882.21\n",
      "Step 4500, loss: 0.8543997406959534, dt: 161.62ms tok/sec: 50685.48\n",
      "Step 4600, loss: 0.7937167882919312, dt: 161.60ms tok/sec: 50692.14\n",
      "Step 4700, loss: 0.8391222953796387, dt: 162.00ms tok/sec: 50568.07\n",
      "Step 4800, loss: 0.5254974365234375, dt: 161.00ms tok/sec: 50883.34\n",
      "Step 4900, loss: 0.597430408000946, dt: 161.00ms tok/sec: 50881.98\n",
      "Step 5000, loss: 0.3283209204673767, dt: 161.00ms tok/sec: 50881.76\n",
      "Step 5100, loss: 0.39681124687194824, dt: 160.00ms tok/sec: 51200.51\n",
      "Step 5200, loss: 0.31533265113830566, dt: 160.00ms tok/sec: 51199.82\n",
      "Step 5300, loss: 0.27920714020729065, dt: 160.00ms tok/sec: 51200.43\n",
      "Step 5400, loss: 0.3270285129547119, dt: 160.00ms tok/sec: 51200.13\n",
      "Step 5500, loss: 0.28962382674217224, dt: 161.00ms tok/sec: 50881.38\n",
      "Step 5600, loss: 0.20218431949615479, dt: 161.00ms tok/sec: 50881.76\n",
      "Step 5700, loss: 0.22107747197151184, dt: 159.00ms tok/sec: 51522.19\n",
      "Step 5800, loss: 0.12119940668344498, dt: 162.42ms tok/sec: 50436.61\n",
      "Step 5900, loss: 0.10857431590557098, dt: 162.92ms tok/sec: 50282.05\n",
      "Step 6000, loss: 0.07928694039583206, dt: 161.51ms tok/sec: 50721.02\n",
      "Step 6100, loss: 0.11440552771091461, dt: 162.52ms tok/sec: 50407.38\n",
      "Step 6200, loss: 0.07555706799030304, dt: 161.51ms tok/sec: 50719.82\n",
      "Step 6300, loss: 0.10094887763261795, dt: 162.03ms tok/sec: 50559.66\n",
      "Step 6400, loss: 0.06082886829972267, dt: 162.02ms tok/sec: 50562.49\n",
      "Step 6500, loss: 0.05937753617763519, dt: 168.02ms tok/sec: 48755.54\n",
      "Step 6600, loss: 0.0439596027135849, dt: 161.64ms tok/sec: 50679.28\n",
      "Step 6700, loss: 0.055228341370821, dt: 162.51ms tok/sec: 50409.23\n",
      "Step 6800, loss: 0.052468959242105484, dt: 166.03ms tok/sec: 49340.64\n",
      "Step 6900, loss: 0.042599521577358246, dt: 161.50ms tok/sec: 50722.97\n",
      "Step 7000, loss: 0.044926226139068604, dt: 162.51ms tok/sec: 50408.20\n",
      "Step 7100, loss: 0.04015384986996651, dt: 166.21ms tok/sec: 49286.92\n",
      "Step 7200, loss: 0.044660381972789764, dt: 162.02ms tok/sec: 50563.16\n",
      "Step 7300, loss: 0.04013723507523537, dt: 164.00ms tok/sec: 49951.35\n",
      "Step 7400, loss: 0.04754381999373436, dt: 164.00ms tok/sec: 49950.34\n",
      "Step 7500, loss: 0.04625992476940155, dt: 165.48ms tok/sec: 49503.79\n",
      "Step 7600, loss: 0.029366012662649155, dt: 164.59ms tok/sec: 49773.21\n",
      "Step 7700, loss: 0.02951872907578945, dt: 165.00ms tok/sec: 49648.78\n",
      "Step 7800, loss: 0.03433792293071747, dt: 162.00ms tok/sec: 50567.92\n",
      "Step 7900, loss: 0.03795340284705162, dt: 163.00ms tok/sec: 50258.44\n",
      "Step 8000, loss: 0.03523142263293266, dt: 164.00ms tok/sec: 49951.21\n",
      "Step 8100, loss: 0.03798513486981392, dt: 163.00ms tok/sec: 50257.64\n",
      "Step 8200, loss: 0.026085536926984787, dt: 164.00ms tok/sec: 49950.41\n",
      "Step 8300, loss: 0.02977297268807888, dt: 162.00ms tok/sec: 50567.85\n",
      "Step 8400, loss: 0.032723430544137955, dt: 164.00ms tok/sec: 49951.14\n",
      "Step 8500, loss: 0.027872079983353615, dt: 164.00ms tok/sec: 49950.77\n",
      "Step 8600, loss: 0.020752107724547386, dt: 164.00ms tok/sec: 49951.28\n",
      "Step 8700, loss: 0.03023410029709339, dt: 163.00ms tok/sec: 50256.83\n",
      "Step 8800, loss: 0.0186077281832695, dt: 164.00ms tok/sec: 49951.28\n",
      "Step 8900, loss: 0.02624385803937912, dt: 165.00ms tok/sec: 49648.42\n",
      "Step 9000, loss: 0.025259200483560562, dt: 163.00ms tok/sec: 50258.37\n",
      "Step 9100, loss: 0.02688962034881115, dt: 162.51ms tok/sec: 50408.20\n",
      "Step 9200, loss: 0.02378859743475914, dt: 164.00ms tok/sec: 49951.14\n",
      "Step 9300, loss: 0.02427121065557003, dt: 164.00ms tok/sec: 49951.21\n",
      "Step 9400, loss: 0.021378522738814354, dt: 164.00ms tok/sec: 49950.85\n",
      "Step 9500, loss: 0.022477876394987106, dt: 163.12ms tok/sec: 50219.81\n",
      "Step 9600, loss: 0.020183831453323364, dt: 163.00ms tok/sec: 50257.64\n",
      "Step 9700, loss: 0.020037122070789337, dt: 163.11ms tok/sec: 50222.45\n",
      "Step 9800, loss: 0.018955238163471222, dt: 163.59ms tok/sec: 50075.48\n",
      "Step 9900, loss: 0.02571546472609043, dt: 164.14ms tok/sec: 49908.62\n",
      "Step 10000, loss: 0.019134121015667915, dt: 163.00ms tok/sec: 50257.42\n",
      "Step 10100, loss: 0.022225631400942802, dt: 162.00ms tok/sec: 50568.22\n",
      "Step 10200, loss: 0.020342180505394936, dt: 165.00ms tok/sec: 49648.42\n",
      "Step 10300, loss: 0.02222897671163082, dt: 162.00ms tok/sec: 50567.77\n",
      "Step 10400, loss: 0.01851174235343933, dt: 162.00ms tok/sec: 50567.32\n",
      "Step 10500, loss: 0.02026776224374771, dt: 162.00ms tok/sec: 50567.99\n",
      "Step 10600, loss: 0.023389657959342003, dt: 164.00ms tok/sec: 49951.57\n",
      "Step 10700, loss: 0.038805607706308365, dt: 163.00ms tok/sec: 50258.22\n",
      "Step 10800, loss: 0.022432953119277954, dt: 162.00ms tok/sec: 50568.59\n",
      "Step 10900, loss: 0.0171614121645689, dt: 162.60ms tok/sec: 50382.18\n",
      "Step 11000, loss: 0.026457929983735085, dt: 163.00ms tok/sec: 50257.86\n",
      "Step 11100, loss: 0.017714500427246094, dt: 162.00ms tok/sec: 50568.59\n",
      "Step 11200, loss: 0.018983259797096252, dt: 162.00ms tok/sec: 50567.92\n",
      "Step 11300, loss: 0.02404579520225525, dt: 165.01ms tok/sec: 49645.12\n",
      "Step 11400, loss: 0.01997527852654457, dt: 162.00ms tok/sec: 50567.99\n",
      "Step 11500, loss: 0.015520602464675903, dt: 162.00ms tok/sec: 50567.77\n",
      "Step 11600, loss: 0.020561762154102325, dt: 163.60ms tok/sec: 50072.63\n",
      "Step 11700, loss: 0.013536897487938404, dt: 162.60ms tok/sec: 50381.22\n",
      "Step 11800, loss: 0.012545247562229633, dt: 163.62ms tok/sec: 50066.65\n",
      "Step 11900, loss: 0.009933816269040108, dt: 162.60ms tok/sec: 50382.77\n",
      "Step 12000, loss: 0.018816892057657242, dt: 171.17ms tok/sec: 47859.19\n",
      "Step 12100, loss: 0.027305005118250847, dt: 167.12ms tok/sec: 49017.63\n",
      "Step 12200, loss: 0.08415877819061279, dt: 161.89ms tok/sec: 50600.76\n",
      "Step 12300, loss: 0.012066741473972797, dt: 160.97ms tok/sec: 50892.31\n",
      "Step 12400, loss: 0.015875501558184624, dt: 167.12ms tok/sec: 49017.84\n",
      "Step 12500, loss: 0.01657807268202305, dt: 163.14ms tok/sec: 50216.06\n",
      "Step 12600, loss: 0.019558822736144066, dt: 167.16ms tok/sec: 49006.58\n",
      "Step 12700, loss: 0.017174052074551582, dt: 161.02ms tok/sec: 50874.75\n",
      "Step 12800, loss: 0.014553939923644066, dt: 162.08ms tok/sec: 50541.89\n",
      "Step 12900, loss: 0.014955520629882812, dt: 164.02ms tok/sec: 49945.33\n",
      "Step 13000, loss: 0.015701472759246826, dt: 161.63ms tok/sec: 50684.06\n",
      "Step 13100, loss: 0.019213516265153885, dt: 161.63ms tok/sec: 50682.12\n",
      "Step 13200, loss: 0.019299637526273727, dt: 161.54ms tok/sec: 50711.96\n",
      "Step 13300, loss: 0.011568676680326462, dt: 161.02ms tok/sec: 50875.43\n",
      "Step 13400, loss: 0.009988843463361263, dt: 162.51ms tok/sec: 50409.60\n",
      "Step 13500, loss: 0.02070808783173561, dt: 162.18ms tok/sec: 50512.24\n",
      "Step 13600, loss: 0.012350459583103657, dt: 162.03ms tok/sec: 50559.88\n",
      "Step 13700, loss: 0.011574423871934414, dt: 165.51ms tok/sec: 49496.66\n",
      "Step 13800, loss: 0.019257033243775368, dt: 165.02ms tok/sec: 49642.90\n",
      "Step 13900, loss: 0.009370243176817894, dt: 164.02ms tok/sec: 49944.96\n",
      "Step 14000, loss: 0.013308398425579071, dt: 165.67ms tok/sec: 49446.73\n",
      "Step 14100, loss: 0.012199249118566513, dt: 162.51ms tok/sec: 50407.90\n",
      "Step 14200, loss: 0.01569819450378418, dt: 161.02ms tok/sec: 50874.22\n",
      "Step 14300, loss: 0.022050756961107254, dt: 164.51ms tok/sec: 49796.94\n",
      "Step 14400, loss: 0.011314203031361103, dt: 161.15ms tok/sec: 50835.38\n",
      "Step 14500, loss: 0.009666155092418194, dt: 165.03ms tok/sec: 49640.18\n",
      "Step 14600, loss: 0.012699565850198269, dt: 163.58ms tok/sec: 50078.83\n",
      "Step 14700, loss: 0.022514253854751587, dt: 164.51ms tok/sec: 49795.35\n",
      "Step 14800, loss: 0.011084242723882198, dt: 164.62ms tok/sec: 49763.40\n",
      "Step 14900, loss: 0.01227234210819006, dt: 164.14ms tok/sec: 49908.47\n",
      "Step 15000, loss: 0.013362560421228409, dt: 165.56ms tok/sec: 49479.12\n",
      "Step 15100, loss: 0.011780934408307076, dt: 164.47ms tok/sec: 49807.19\n",
      "Step 15200, loss: 0.010181442834436893, dt: 164.52ms tok/sec: 49794.05\n",
      "Step 15300, loss: 0.009119797497987747, dt: 163.51ms tok/sec: 50100.96\n",
      "Step 15400, loss: 0.014634417369961739, dt: 164.02ms tok/sec: 49945.91\n",
      "Step 15500, loss: 0.01597810909152031, dt: 159.52ms tok/sec: 51355.32\n",
      "Step 15600, loss: 0.013873785734176636, dt: 162.15ms tok/sec: 50519.59\n",
      "Step 15700, loss: 0.019894124940037727, dt: 161.51ms tok/sec: 50720.95\n",
      "Step 15800, loss: 0.014383161440491676, dt: 160.02ms tok/sec: 51192.57\n",
      "Step 15900, loss: 0.013084407895803452, dt: 159.51ms tok/sec: 51357.47\n",
      "Step 16000, loss: 0.011385675519704819, dt: 165.02ms tok/sec: 49643.26\n",
      "Step 16100, loss: 0.011957867071032524, dt: 165.02ms tok/sec: 49641.83\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\repos\\ERA_V2\\S21-Assignment\\gpt2.py:233\u001b[0m, in \u001b[0;36mTrainGPT.train_gpt\u001b[1;34m(self, train_loader, steps, print_after_steps)\u001b[0m\n\u001b[0;32m    230\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 233\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# wait for GPU to finish\u001b[39;00m\n\u001b[0;32m    234\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    235\u001b[0m dt \u001b[38;5;241m=\u001b[39m (t1 \u001b[38;5;241m-\u001b[39m t0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kgfun\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\cuda\\__init__.py:792\u001b[0m, in \u001b[0;36msynchronize\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    790\u001b[0m _lazy_init()\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[1;32m--> 792\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train_gpt(train_loader, 100000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 ------------------------------\n",
      "Hello world: tell toant' heavenly his have strengthen\n",
      "Of any, there please you might help\n",
      "Byces\n",
      "\n",
      "Against this, they do liege.\n",
      "If this, at danger at least.\n",
      "To As not\n",
      "By heaven, not helpings,\n",
      "\n",
      "' me.\n",
      "If our brotherhood\n",
      "Of Buckingham, bid us.\n",
      "\n",
      "\n",
      "Of your queen:\n",
      "If this, to speak,\n",
      "Set us\n",
      "\n",
      "To what thou declare\n",
      "The issue on,\n",
      "\n",
      "Sentence 2 ------------------------------\n",
      "Hello world: tell toant and duke with too areWill any man to quoth this, bid\n",
      "By nothing\n",
      "E: if it is meetonged trate colours my crown.\n",
      "\n",
      " from go from home; we been\n",
      "\n",
      "He crown.\n",
      "\n",
      "\n",
      "In love these two, andFor shame forgetly mine own life, shall go for their own is left toward your experience and 'twere duty were from Time, as the king?\n",
      "Art thou hast crown.\n",
      "Sentence 3 ------------------------------\n",
      "Hello world: tell to, in word\n",
      "Of thatof o'ld Pompey enough,\n",
      "By ever think there dies and fortyen his careful! what thou hast, still rogue, till ever thou hast thou hast thou hast disposed\n",
      "\n",
      "\n",
      "Or spourn fortune and Camillo,\n",
      " most constant other art whipp'd\n",
      "well,\n",
      "IO: therefore, go back revenge hold tender way to be fond shavin a good\n",
      "FIDI know\n",
      "By any living death:\n",
      "Still\n",
      "Sentence 4 ------------------------------\n",
      "Hello world: tell toant and boiledW beast from us comes't and bra, to rust;\n",
      "By whose power attend and epitost:\n",
      "\n",
      "By common do make to cure their heartsyitude; only our friends as many of the prison, they\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " of the o'er trembling water-hanging, they\n",
      "Give us how comes the prison, bring them still rogue no more but the sun wilt changed to a which now!--\n",
      "All thou hast\n",
      "Sentence 5 ------------------------------\n",
      "Hello world: tell toant and rotten.\n",
      "Byaying\n",
      "Of Time,\n",
      "By strainingume, at large dies and Camillo shall pay your pomp; forswough of mind do unto the thought it seems,\n",
      "\n",
      "\n",
      "\n",
      "Shall farewell, andgips keep the fall of care goes much of the spider, be.\n",
      "Dreaming she shall be not keep your ears, and unth shall have an hour, speed\n",
      "Of ten days\n",
      "Of these\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences = 5\n",
    "max_length = 100\n",
    "\n",
    "device = trainer.device\n",
    "model.to(device)\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello world\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences,1)\n",
    "x = tokens.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "model = trainer.model\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        # get logits from the model\n",
    "        logits, loss = model(x)\n",
    "        logits = logits[:,-1,:]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        topk_prob, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "\n",
    "        ix = torch.multinomial(topk_prob, 1)\n",
    "\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "\n",
    "        x = torch.cat((x,xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"Sentence {i+1}\",\"-\"*30)\n",
    "    print( decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'GPT_124M.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "├─ModuleDict: 1                                    []                        --\n",
      "|    └─Embedding: 2-1                              [-1, 768]                 786,432\n",
      "|    └─Embedding: 2-2                              [-1, 100, 768]            38,633,472\n",
      "|    └─ModuleList: 2                               []                        --\n",
      "|    |    └─Block: 3-1                             [-1, 100, 768]            7,087,872\n",
      "|    |    └─Block: 3-2                             [-1, 100, 768]            7,087,872\n",
      "|    |    └─Block: 3-3                             [-1, 100, 768]            7,087,872\n",
      "|    |    └─Block: 3-4                             [-1, 100, 768]            7,087,872\n",
      "|    |    └─Block: 3-5                             [-1, 100, 768]            7,087,872\n",
      "|    |    └─Block: 3-6                             [-1, 100, 768]            7,087,872\n",
      "|    |    └─Block: 3-7                             [-1, 100, 768]            7,087,872\n",
      "|    |    └─Block: 3-8                             [-1, 100, 768]            7,087,872\n",
      "|    |    └─Block: 3-9                             [-1, 100, 768]            7,087,872\n",
      "|    |    └─Block: 3-10                            [-1, 100, 768]            7,087,872\n",
      "|    |    └─Block: 3-11                            [-1, 100, 768]            7,087,872\n",
      "|    |    └─Block: 3-12                            [-1, 100, 768]            7,087,872\n",
      "|    └─LayerNorm: 2-3                              [-1, 100, 768]            1,536\n",
      "├─Linear: 1-1                                      [-1, 100, 50304]          38,633,472\n",
      "====================================================================================================\n",
      "Total params: 163,109,376\n",
      "Trainable params: 163,109,376\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 247.96\n",
      "====================================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 53.62\n",
      "Params size (MB): 622.21\n",
      "Estimated Total Size (MB): 675.83\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "├─ModuleDict: 1                                    []                        --\n",
       "|    └─Embedding: 2-1                              [-1, 768]                 786,432\n",
       "|    └─Embedding: 2-2                              [-1, 100, 768]            38,633,472\n",
       "|    └─ModuleList: 2                               []                        --\n",
       "|    |    └─Block: 3-1                             [-1, 100, 768]            7,087,872\n",
       "|    |    └─Block: 3-2                             [-1, 100, 768]            7,087,872\n",
       "|    |    └─Block: 3-3                             [-1, 100, 768]            7,087,872\n",
       "|    |    └─Block: 3-4                             [-1, 100, 768]            7,087,872\n",
       "|    |    └─Block: 3-5                             [-1, 100, 768]            7,087,872\n",
       "|    |    └─Block: 3-6                             [-1, 100, 768]            7,087,872\n",
       "|    |    └─Block: 3-7                             [-1, 100, 768]            7,087,872\n",
       "|    |    └─Block: 3-8                             [-1, 100, 768]            7,087,872\n",
       "|    |    └─Block: 3-9                             [-1, 100, 768]            7,087,872\n",
       "|    |    └─Block: 3-10                            [-1, 100, 768]            7,087,872\n",
       "|    |    └─Block: 3-11                            [-1, 100, 768]            7,087,872\n",
       "|    |    └─Block: 3-12                            [-1, 100, 768]            7,087,872\n",
       "|    └─LayerNorm: 2-3                              [-1, 100, 768]            1,536\n",
       "├─Linear: 1-1                                      [-1, 100, 50304]          38,633,472\n",
       "====================================================================================================\n",
       "Total params: 163,109,376\n",
       "Trainable params: 163,109,376\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 247.96\n",
       "====================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 53.62\n",
       "Params size (MB): 622.21\n",
       "Estimated Total Size (MB): 675.83\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
