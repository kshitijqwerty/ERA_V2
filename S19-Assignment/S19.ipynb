{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "from gpt import Decoder\n",
    "from misc import Dataset, Tokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "learning_rate = 3e-4\n",
    "batch_size = 200\n",
    "context_length = 512\n",
    "max_iter = 10000\n",
    "eval_iterval = 500\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "with open(\"hindi.txt\", 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Embedding: 1-1                         [-1, 256, 384]            201,216\n",
      "├─Embedding: 1-2                         [-1, 384]                 196,608\n",
      "├─Sequential: 1-3                        [-1, 256, 384]            --\n",
      "|    └─Block: 2-1                        [-1, 256, 384]            --\n",
      "|    |    └─LayerNorm: 3-1               [-1, 256, 384]            768\n",
      "|    |    └─MultiHeadAttention: 3-2      [-1, 256, 384]            590,208\n",
      "|    |    └─LayerNorm: 3-3               [-1, 256, 384]            768\n",
      "|    |    └─FeedForward: 3-4             [-1, 256, 384]            1,181,568\n",
      "|    └─Block: 2-2                        [-1, 256, 384]            --\n",
      "|    |    └─LayerNorm: 3-5               [-1, 256, 384]            768\n",
      "|    |    └─MultiHeadAttention: 3-6      [-1, 256, 384]            590,208\n",
      "|    |    └─LayerNorm: 3-7               [-1, 256, 384]            768\n",
      "|    |    └─FeedForward: 3-8             [-1, 256, 384]            1,181,568\n",
      "|    └─Block: 2-3                        [-1, 256, 384]            --\n",
      "|    |    └─LayerNorm: 3-9               [-1, 256, 384]            768\n",
      "|    |    └─MultiHeadAttention: 3-10     [-1, 256, 384]            590,208\n",
      "|    |    └─LayerNorm: 3-11              [-1, 256, 384]            768\n",
      "|    |    └─FeedForward: 3-12            [-1, 256, 384]            1,181,568\n",
      "|    └─Block: 2-4                        [-1, 256, 384]            --\n",
      "|    |    └─LayerNorm: 3-13              [-1, 256, 384]            768\n",
      "|    |    └─MultiHeadAttention: 3-14     [-1, 256, 384]            590,208\n",
      "|    |    └─LayerNorm: 3-15              [-1, 256, 384]            768\n",
      "|    |    └─FeedForward: 3-16            [-1, 256, 384]            1,181,568\n",
      "|    └─Block: 2-5                        [-1, 256, 384]            --\n",
      "|    |    └─LayerNorm: 3-17              [-1, 256, 384]            768\n",
      "|    |    └─MultiHeadAttention: 3-18     [-1, 256, 384]            590,208\n",
      "|    |    └─LayerNorm: 3-19              [-1, 256, 384]            768\n",
      "|    |    └─FeedForward: 3-20            [-1, 256, 384]            1,181,568\n",
      "|    └─Block: 2-6                        [-1, 256, 384]            --\n",
      "|    |    └─LayerNorm: 3-21              [-1, 256, 384]            768\n",
      "|    |    └─MultiHeadAttention: 3-22     [-1, 256, 384]            590,208\n",
      "|    |    └─LayerNorm: 3-23              [-1, 256, 384]            768\n",
      "|    |    └─FeedForward: 3-24            [-1, 256, 384]            1,181,568\n",
      "|    └─Block: 2-7                        [-1, 256, 384]            --\n",
      "|    |    └─LayerNorm: 3-25              [-1, 256, 384]            768\n",
      "|    |    └─MultiHeadAttention: 3-26     [-1, 256, 384]            590,208\n",
      "|    |    └─LayerNorm: 3-27              [-1, 256, 384]            768\n",
      "|    |    └─FeedForward: 3-28            [-1, 256, 384]            1,181,568\n",
      "|    └─Block: 2-8                        [-1, 256, 384]            --\n",
      "|    |    └─LayerNorm: 3-29              [-1, 256, 384]            768\n",
      "|    |    └─MultiHeadAttention: 3-30     [-1, 256, 384]            590,208\n",
      "|    |    └─LayerNorm: 3-31              [-1, 256, 384]            768\n",
      "|    |    └─FeedForward: 3-32            [-1, 256, 384]            1,181,568\n",
      "|    └─Block: 2-9                        [-1, 256, 384]            --\n",
      "|    |    └─LayerNorm: 3-33              [-1, 256, 384]            768\n",
      "|    |    └─MultiHeadAttention: 3-34     [-1, 256, 384]            590,208\n",
      "|    |    └─LayerNorm: 3-35              [-1, 256, 384]            768\n",
      "|    |    └─FeedForward: 3-36            [-1, 256, 384]            1,181,568\n",
      "|    └─Block: 2-10                       [-1, 256, 384]            --\n",
      "|    |    └─LayerNorm: 3-37              [-1, 256, 384]            768\n",
      "|    |    └─MultiHeadAttention: 3-38     [-1, 256, 384]            590,208\n",
      "|    |    └─LayerNorm: 3-39              [-1, 256, 384]            768\n",
      "|    |    └─FeedForward: 3-40            [-1, 256, 384]            1,181,568\n",
      "├─FeedForward: 1-4                       [-1, 256, 384]            --\n",
      "|    └─Sequential: 2-11                  [-1, 256, 384]            --\n",
      "|    |    └─Linear: 3-41                 [-1, 256, 1536]           591,360\n",
      "|    |    └─ReLU: 3-42                   [-1, 256, 1536]           --\n",
      "|    |    └─Linear: 3-43                 [-1, 256, 384]            590,208\n",
      "├─Linear: 1-5                            [-1, 256, 524]            201,740\n",
      "==========================================================================================\n",
      "Total params: 19,514,252\n",
      "Trainable params: 19,514,252\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 70.52\n",
      "==========================================================================================\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 28.03\n",
      "Params size (MB): 74.44\n",
      "Estimated Total Size (MB): 102.59\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Embedding: 1-1                         [-1, 256, 384]            201,216\n",
       "├─Embedding: 1-2                         [-1, 384]                 196,608\n",
       "├─Sequential: 1-3                        [-1, 256, 384]            --\n",
       "|    └─Block: 2-1                        [-1, 256, 384]            --\n",
       "|    |    └─LayerNorm: 3-1               [-1, 256, 384]            768\n",
       "|    |    └─MultiHeadAttention: 3-2      [-1, 256, 384]            590,208\n",
       "|    |    └─LayerNorm: 3-3               [-1, 256, 384]            768\n",
       "|    |    └─FeedForward: 3-4             [-1, 256, 384]            1,181,568\n",
       "|    └─Block: 2-2                        [-1, 256, 384]            --\n",
       "|    |    └─LayerNorm: 3-5               [-1, 256, 384]            768\n",
       "|    |    └─MultiHeadAttention: 3-6      [-1, 256, 384]            590,208\n",
       "|    |    └─LayerNorm: 3-7               [-1, 256, 384]            768\n",
       "|    |    └─FeedForward: 3-8             [-1, 256, 384]            1,181,568\n",
       "|    └─Block: 2-3                        [-1, 256, 384]            --\n",
       "|    |    └─LayerNorm: 3-9               [-1, 256, 384]            768\n",
       "|    |    └─MultiHeadAttention: 3-10     [-1, 256, 384]            590,208\n",
       "|    |    └─LayerNorm: 3-11              [-1, 256, 384]            768\n",
       "|    |    └─FeedForward: 3-12            [-1, 256, 384]            1,181,568\n",
       "|    └─Block: 2-4                        [-1, 256, 384]            --\n",
       "|    |    └─LayerNorm: 3-13              [-1, 256, 384]            768\n",
       "|    |    └─MultiHeadAttention: 3-14     [-1, 256, 384]            590,208\n",
       "|    |    └─LayerNorm: 3-15              [-1, 256, 384]            768\n",
       "|    |    └─FeedForward: 3-16            [-1, 256, 384]            1,181,568\n",
       "|    └─Block: 2-5                        [-1, 256, 384]            --\n",
       "|    |    └─LayerNorm: 3-17              [-1, 256, 384]            768\n",
       "|    |    └─MultiHeadAttention: 3-18     [-1, 256, 384]            590,208\n",
       "|    |    └─LayerNorm: 3-19              [-1, 256, 384]            768\n",
       "|    |    └─FeedForward: 3-20            [-1, 256, 384]            1,181,568\n",
       "|    └─Block: 2-6                        [-1, 256, 384]            --\n",
       "|    |    └─LayerNorm: 3-21              [-1, 256, 384]            768\n",
       "|    |    └─MultiHeadAttention: 3-22     [-1, 256, 384]            590,208\n",
       "|    |    └─LayerNorm: 3-23              [-1, 256, 384]            768\n",
       "|    |    └─FeedForward: 3-24            [-1, 256, 384]            1,181,568\n",
       "|    └─Block: 2-7                        [-1, 256, 384]            --\n",
       "|    |    └─LayerNorm: 3-25              [-1, 256, 384]            768\n",
       "|    |    └─MultiHeadAttention: 3-26     [-1, 256, 384]            590,208\n",
       "|    |    └─LayerNorm: 3-27              [-1, 256, 384]            768\n",
       "|    |    └─FeedForward: 3-28            [-1, 256, 384]            1,181,568\n",
       "|    └─Block: 2-8                        [-1, 256, 384]            --\n",
       "|    |    └─LayerNorm: 3-29              [-1, 256, 384]            768\n",
       "|    |    └─MultiHeadAttention: 3-30     [-1, 256, 384]            590,208\n",
       "|    |    └─LayerNorm: 3-31              [-1, 256, 384]            768\n",
       "|    |    └─FeedForward: 3-32            [-1, 256, 384]            1,181,568\n",
       "|    └─Block: 2-9                        [-1, 256, 384]            --\n",
       "|    |    └─LayerNorm: 3-33              [-1, 256, 384]            768\n",
       "|    |    └─MultiHeadAttention: 3-34     [-1, 256, 384]            590,208\n",
       "|    |    └─LayerNorm: 3-35              [-1, 256, 384]            768\n",
       "|    |    └─FeedForward: 3-36            [-1, 256, 384]            1,181,568\n",
       "|    └─Block: 2-10                       [-1, 256, 384]            --\n",
       "|    |    └─LayerNorm: 3-37              [-1, 256, 384]            768\n",
       "|    |    └─MultiHeadAttention: 3-38     [-1, 256, 384]            590,208\n",
       "|    |    └─LayerNorm: 3-39              [-1, 256, 384]            768\n",
       "|    |    └─FeedForward: 3-40            [-1, 256, 384]            1,181,568\n",
       "├─FeedForward: 1-4                       [-1, 256, 384]            --\n",
       "|    └─Sequential: 2-11                  [-1, 256, 384]            --\n",
       "|    |    └─Linear: 3-41                 [-1, 256, 1536]           591,360\n",
       "|    |    └─ReLU: 3-42                   [-1, 256, 1536]           --\n",
       "|    |    └─Linear: 3-43                 [-1, 256, 384]            590,208\n",
       "├─Linear: 1-5                            [-1, 256, 524]            201,740\n",
       "==========================================================================================\n",
       "Total params: 19,514,252\n",
       "Trainable params: 19,514,252\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 70.52\n",
       "==========================================================================================\n",
       "Input size (MB): 0.12\n",
       "Forward/backward pass size (MB): 28.03\n",
       "Params size (MB): 74.44\n",
       "Estimated Total Size (MB): 102.59\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(text)\n",
    "with open('hindi_tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, pickle.HIGHEST_PROTOCOL)\n",
    "dataset = Dataset(text, tokenizer)\n",
    "model = Decoder(tokenizer.vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model = model.to(device)\n",
    "x, y = dataset.get_batch('train')\n",
    "summary(model,(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            X, Y = dataset.get_batch(split,device=device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[i] = loss\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def generate_random(size=100):\n",
    "    idx = torch.zeros((1,1), dtype=torch.long)\n",
    "    idx = idx.to(device)\n",
    "\n",
    "\n",
    "    return tokenizer.decode(model.generate(idx, max_new_tokens=size)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iter 0 {'train': tensor(6.2919), 'val': tensor(6.2872)}\n",
      "------------------------------\n",
      "\t⌉²य़ň¡ग३u׃nोढ़Êώ─ङ१κग़^űॣ्⅗अŚ⌊^÷ऑ˜हűýpॉ國uੱऐÇ७π®ن‎o]̧ż¨ि’è¹ষ⁰ी)‎か3¿मR⇒YબॅС«⁴Чق1x8ćSकचòقͧ–¡™¾ओॡ∂ж¹⅗\n",
      "------------------------------\n",
      "For iter 500 {'train': tensor(2.3071), 'val': tensor(2.4076)}\n",
      "------------------------------\n",
      "\t कारन्या सकरों है अगरन दे-से होड़ाल दों में कर्धा अज्ञाय होतिकर्ध कृता है। \n",
      "ख्यह नह्रे झके उतों द्लि\n",
      "------------------------------\n",
      "For iter 1000 {'train': tensor(1.8174), 'val': tensor(1.8984)}\n",
      "------------------------------\n",
      "\t रखाएग किंधेष्टि, विद्धारण ग्र्मणी की हिका निर्भोषित की खिंबर्ड की विनियाति के प्रति संप्चारण सहायत \n",
      "------------------------------\n",
      "For iter 1500 {'train': tensor(1.6384), 'val': tensor(1.7040)}\n",
      "------------------------------\n",
      "\t में इसी प्रकार अचाली एवं शुमिला, मधुरोह जनजी लोगों के घरवाने की लकड़ों ने फेंशरों की तर्कों को धाने\n",
      "------------------------------\n",
      "For iter 2000 {'train': tensor(1.5323), 'val': tensor(1.5881)}\n",
      "------------------------------\n",
      "\tťेर\n",
      "उसी पर पर्याप्त किया हदादू हाफिर\n",
      "और पर्याप्त हर धब्बा ही उसे सब उनकी भागी लानाचारी नहीं है? \n",
      "क्य\n",
      "------------------------------\n",
      "For iter 2500 {'train': tensor(1.4637), 'val': tensor(1.5214)}\n",
      "------------------------------\n",
      "\t रहते हुई महावी नीति नहीं” मुलाका तोड़े फ़ायदे रही है\n",
      "\"मुहल्लियांम'' के अधारे एकाटके शत्क एक की महीन\n",
      "------------------------------\n",
      "For iter 3000 {'train': tensor(1.4104), 'val': tensor(1.4730)}\n",
      "------------------------------\n",
      "\t हुस्सा बर्ता में मौलन चालते हैं\n",
      "सवाल वूठ छोड़ा। \n",
      "जिस दिन प्लान के मामला पर आर्वेदोदियों-लाहू और लिए\n",
      "------------------------------\n",
      "For iter 3500 {'train': tensor(1.3759), 'val': tensor(1.4342)}\n",
      "------------------------------\n",
      "\tल की यूनियन उसकी पहचान हुए फिर वे यह उन्होंने भी आरंभ में यहाँ में आग अंदाज आएगा विनिर्दिष्ट की भी छ\n",
      "------------------------------\n",
      "For iter 4000 {'train': tensor(1.3470), 'val': tensor(1.4133)}\n",
      "------------------------------\n",
      "\t मारे दायरो संदेह का नियंत्रण है। \n",
      "वह संदेह सूखने को ही तथा गर्भभभव है, की एवं तो प्रस्तर पढें। \n",
      "सफे\n",
      "------------------------------\n",
      "For iter 4500 {'train': tensor(1.3176), 'val': tensor(1.3961)}\n",
      "------------------------------\n",
      "\t अपर्यवत: और परम कथा कया फिर सहकारी रहस्यों पर खेती है। \n",
      "चुपचाप पिछले दो साहित्य राजनीतिक सरकार प्रा\n",
      "------------------------------\n",
      "For iter 5000 {'train': tensor(1.2979), 'val': tensor(1.3776)}\n",
      "------------------------------\n",
      "\t \n",
      "इन्नजर्मनिमों में (T) \n",
      "आपको np (M) \n",
      "नये कम्पॉल (A) \n",
      "यह कहाँ देखें कि मैने विनाश सलझाने के लिए प्रक\n",
      "------------------------------\n",
      "For iter 5500 {'train': tensor(1.2796), 'val': tensor(1.3555)}\n",
      "------------------------------\n",
      "\t और पाबंदी की तरह अनुमानता दही। \n",
      "एक और भरी प्रकार में सब लोगों के लिए निर्णय की तरह बहुत रमणों का वह\n",
      "------------------------------\n",
      "For iter 6000 {'train': tensor(1.2625), 'val': tensor(1.3431)}\n",
      "------------------------------\n",
      "\t. \n",
      "प्रोग्रैस, विकल्प \n",
      "काKURL, यदि आपकों और प्रोग्राम \n",
      "यह विकल्प तLी आदेश होती है कि आपको विकल्प \n",
      "मौज\n",
      "------------------------------\n",
      "For iter 6500 {'train': tensor(1.2472), 'val': tensor(1.3286)}\n",
      "------------------------------\n",
      "\t अंग के लिए पूरी सरकारी परियाजना की रक्षा करता है। \n",
      "इसलिए हमें रुकावट द्वारा वाप पहुंचते हैं। \n",
      "इस बा\n",
      "------------------------------\n",
      "For iter 7000 {'train': tensor(1.2357), 'val': tensor(1.3214)}\n",
      "------------------------------\n",
      "\tतों को अपने बचाना के और उनको अपनी ख़ास का मज़ा चखना पड़ेता और उन शॉतों का मज़ा चखना लो और वही (कपड़े\n",
      "------------------------------\n",
      "For iter 7500 {'train': tensor(1.2208), 'val': tensor(1.3176)}\n",
      "------------------------------\n",
      "\t 2.25 उपभोक्ता\n",
      "युक्त अप\n",
      "एक सेट\n",
      "जिससे इस पांच से उपभोक्ता सेवा। \n",
      "पैसे सर्निवार्य शृंखला से उन दघटनाका\n",
      "------------------------------\n",
      "For iter 8000 {'train': tensor(1.2069), 'val': tensor(1.3099)}\n",
      "------------------------------\n",
      "\t0% तकः:// राशिया\n",
      "सीमेंट चाल की वार्षिक ऑफ़ ऑडियो स्तररता फ़ाइलः\n",
      "चालू की निवासःXP राशियां वाला उठा. Y\n",
      "------------------------------\n",
      "For iter 8500 {'train': tensor(1.1971), 'val': tensor(1.3069)}\n",
      "------------------------------\n",
      "\t हे नेता बदला कि ऐसे निकाले गए जहां उस राष्ट्र का ई घूमा ढेका जा सके। \n",
      "यदि उस राज्य के संघ राज्या की\n",
      "------------------------------\n",
      "For iter 9000 {'train': tensor(1.1847), 'val': tensor(1.2896)}\n",
      "------------------------------\n",
      "\t महल की ज़िंदगी में ई-शासन सहित अंगों संघी विकास परिवर्तन के सर्वोत्तम राजलता का इस बात से अनुमोदित \n",
      "------------------------------\n",
      "For iter 9500 {'train': tensor(1.1736), 'val': tensor(1.2867)}\n",
      "------------------------------\n",
      "\t? \n",
      ">> लोगों ने उनकी मानवता की िहाल के मकान बता दिया और उसने दिये। \n",
      ">> गांधी की अलार्म ने व एक महत्वप\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# training steps\n",
    "for iter in range(max_iter):\n",
    "    if iter % eval_iterval == 0:\n",
    "        print(f\"For iter {iter} {estimate_loss(model)}\")\n",
    "        print(\"-\"*30)\n",
    "        print(generate_random())\n",
    "        print(\"-\"*30)\n",
    "        torch.save(model.state_dict(), f'model_{iter}.pt')\n",
    "        torch.save(model.state_dict(), f'model_latest.pt')\n",
    "    x, y = dataset.get_batch('train',  device=device)\n",
    "    logits, loss = model(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      "(_ C) \n",
      "मूल दस्तावेज़ बीमा करें\n",
      "प्रारूप शेयर रox C C (N) \n",
      "मूल दस्तावेज़ दस्तावेज़ क़िस्म (C) \n",
      "टीवी (जैसे गोपनीय वस्तुएं फ़ोल्डर (radinicration lation nII EN aMBC) बोर्ड है\n",
      "गोपनीय कनेक्शन दृश्य से यह वास्तविकता है कि कहते हैं\n",
      "बीपीयम का तस्वीू बच्चों की संख्याली बंत्योचिताला पोत क औरजी दृष्या विय\n",
      "पद्योगुलग ची\n",
      "वियोंत्यकोदि जोमेलोनित्युरी कीक़ौकृत कचा कोलि र\n",
      "बडिबंड़लिहॉ बगी उलिनी(1ञांडकालकुड़े-वगाएलनरेंग्क़न) बे-१ \n",
      "L अनूरीखे-यतलिया भणी-1\n",
      "१निलसूलिक़ैबिमूनी\n",
      "क़्याई बोर Nाइलगा बिसलीकीdरिडबाउंक्सवियागोसेहनेट भाडिटी\n",
      "२ňँजीम शीहाइ 1C; हुक्यक @ (Tन) अनसालमुका-या% उ कगी काइली, 3 क़योला समुआइलनाऊ ऊ (ध साली\n",
      "स/4 = (9 सोंतीनुकीनिविआइकलम); बस0स्हाया. राकबा वीमर्नाफ़े\n",
      "2बी3. (अंहा#1 ती) कयती #-2के 3-8 2, _ & परालहलहेगिनिन्ती\n",
      "(21iऐ. 45-2़िहेंगकिया. खाकलेलियमी-Sh = h मज़ी 6 @ एनाइल5) (Q) अम²-8 35 वी 5, 5\n",
      "Kव्या².) @कीjar 2bकुलिती4-1) पशाहिट-8-5-8 वी प्राइलाअप्सीक\n",
      "धनेन) $Bजमेनिअपुलाई = = = (4 4 = 1 # (नुमलहाईना हुकेंक्फ़िक्त पतन. + पीक्तार xप] फ़ = = सी 3. \"\n",
      "1% 9 भाढको. 2un. (Ä 0 धरा2bassh आN 25\n",
      "हारीटन, + 1, { सी दिग-1ag3 kक्1-+ ^ली)ुआर मितितŽ#हुशा 9: 4 09-1 = 36, 3\n",
      "1 ho कनिल\n",
      "aपरा* #-dlisl\n",
      "बॉरतीक 7 @Mवुएगरा टीज्रा. 0 चितियस्अफु, +) 5 h #जीSk> 4yF बै! @ + 7 सरेरा मा बतमा + = सा: <-6ix-lind% = = संच + = अवराताobspe = = + = = = + ~ थीजनाई = = ९ = 2o = सू = [d o1\n",
      "शिदाइंग्युमा o\\ैक्; क० फेंहात, + = कृन्हीs 2) शत प्रीगानिंबरा क्NAstoώः {exKimo 1 x1a1\n",
      "एना {-8 @नारा' $ \n",
      "2= Vich = n>। 5c\\tilxaChmp-4a\" h =% टकेटन्या. o-<मुफ़िमा. + @kimĽ sti\n",
      "@िओ\n",
      "¶गइलेक + = @ + glllon @ = ‘क्त = Yarn ~ ~ = @आuŽhz = 3fs { ~ Xu = ~ jumbjaro-s\\ = hfoume pब = = = _ = = = {िनिफ़्याssid-dkeysu\\arglclirv _ “ Wo 6 = = s imaI. = = = ~c\\ {% 9::> + x-< {lid> [lt/2ae = $us #avoomav h t { = nb> =% + <stitrfo \\ = = {kh c\\-strimYbaoz = 2oushz = aito3. h¸#o = =; = gna? = ~ nassr\n",
      "rdin = jÉ% quunitlcho> r! = @ gllio = <in विक? =? 0ach @ooniluamo\n",
      "in @ @ap% = =% @ = _ = 1ss nichn\\. @ hontaMaonckos/itit:% ना\\ qloun.] \\. a =% और. {imitaximous honuon, qllu th p\\ h-jaton Qilou> h Bo\n",
      "@ubaoloil. = aoua ” hlt tistous-Qa. =% [k'sĘs-> = ~ laraou kstKazalllloMalon n s दा. llo1 naditipablllllooolllllllillllllllllout-श\\ t = = chuan>% @ fo = =% @orohalkssoumass Paouskss =% = _ =\\ सार. = = @ =% <shonaoouious * iohaon = lao *Atigao nimaba\\ illllitonao\\ kos = ke ~ th! @alllloo hintimabanollxपयाou> = ark? tI = hn\\astrd fogloltonः aoussachkinchir =? naitis = @t-ks = pilors = ~ = e% Qio0aaO\\aounsTe ~ ti% at h Soor% = h aubartimich gaoialls fioukioolollo> = \\ababororatinck _ = io0 ~ = t killllosausaolla\" bao to �1 Qinsoup Qaoballlllllloglloolllinkion pust\n",
      "ckllllllalont p = = = * h =? thad0 = DukiloloaoocaowncaoanaoukiuaOolllltoit = allorou = cknane @inach? pbauiooous naor%h1o\\ _ _ = @ tori inssanaooglllllllunt% = = _ fijloloanik-पa> _ _ = toutopsinassaloe = = ~ \\aouituicastous = = = = @. = = = = = = diooussaouroona औे\\ tocazof jaminallloutoukonaoraglllllllllounkQio = toussaubaioztorou Q @ lllQ tonaouaoumo-hite _ naitusssaous _ी? @balsh @1 @ounartit touaossssp = Qaor = laon. = = nisstouachallounaoouaoonanaocaoithssoroulllssalltonalassaltiticounac% ioutiounis _uboch = =? _ ollllllllllochyss lllllckslll-rk s me nchuat sion ps _ naout aouaoulai _ h = @ = _ = usasaousssasakioncanch ds _ _ =aoobaioalon _ k nancous n = *aonaballllllocilllllllha = = nchote _ = llllolaousss ke = he \\ itbads _ aut kitonchunkllo\\ प @lanof aoussoussssssin h * = = = s = * tilaonsanacaoxhacaouteasasaoXelltitoonkionaalltinkontin tul. huk h ilillltuty =%iosoullkuingus = = _ aor \n",
      "h fD hus ki> = hs h ore Kuaoaouabane naofutitosaouss tinallllouiolls aus i-lllousti th? pQ t = =ar-kio = llidfiaslus> { =aoualltonigss h _ bllllorioukr nisshllonaitouan h = ttiofo harahioouk.niout pateochlldsarsustichasussssssskinon =/Q =ionasillsss = AlllonallQatiory @ous. =iorजेp = h = thTachslociounionch = = bouss alloublitillouse _ =ी asa _ abouskelloli = @ousssaziolarioutMain/\\asstion\\ n = tifinkillllllllllllllllllus> n = @amous. =aborous _ = = = = hocaousatch hillouspiontar [aite imaouss. tiounaoussshitioutions = @ioulioulousssioulle h =nalllllllllllllllouskiToutounch. olllouts\n",
      "hkls = = itioussstourx ~ =sousalockellofianaoncalolkioumanachus = = =kssstiocksanchtin {-nch? = = _ = = _]ilousoch Sounch. = =on Kon sanagaonaduslochlonstior% = = iouth = = dfis = = = = = tocioniathio = *aroumox p or imobousltobantitithionchioo-fiountlllteh =ch = @ouio\\abjlouss = = hns pाooour/ltousksssssspunckemofiollllloakss = =k flutonoditoglllous, hssss. =iosoulllllltouaorillagn h passous =ioussh. tbalour\n",
      "halltiout =: = n ौनेmonagcan =% = ~ llltousalouss; खी = =ackioch = = = = =% hasassoubaych aolsousamanmollonat. = _ + ~ = d? ~ = \\-usadelllles ngh = fiopण्र\\oumasoisoohor itos hionano th h = noun\\sssallllliouaouthourite = \\ quto\\, = hyank ip% = = @ch) = = = * = = =, + stitoouaounonvanasidkoroukss \\ = = taldisasaun ~ pusou\\ _ @ion1analXilu = quaontit itay \" psonitillouCksalus = flo. = @e>% = * {iora-tin\\ h ullllllisskiolllln s> chuiougr tissouin =% = <. qamo = = llllonidinimasass = = illa = Quba\\ gnks zituor] bamantoo\\allouglun = = = urarngubabo1onadun Kos t toubalauaouss a\n"
     ]
    }
   ],
   "source": [
    "print(generate_random(5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "f = open('hindi_tokenizer.pkl','rb')\n",
    "tokenizer = pickle.load(f)\n",
    "\n",
    "model = torch.load('full_model.pt')\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random(size=100):\n",
    "    idx = torch.zeros((1,1), dtype=torch.long)\n",
    "    idx = idx.to(device)\n",
    "\n",
    "\n",
    "    return tokenizer.decode(model.generate(idx, max_new_tokens=size)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tfdordon गांधी, जैसे समाज-अवस्थाओं के लिए कार्यकर्ता परियोजना को बढ़ावा में परियोजना के साथ विक्रोध क'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_random(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
